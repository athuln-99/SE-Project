{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4798ae8",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c19e86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "92327ec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.6.0-cp39-cp39-win_amd64.whl (12.3 MB)\n",
      "                                              0.0/12.3 MB ? eta -:--:--\n",
      "                                              0.2/12.3 MB 6.3 MB/s eta 0:00:02\n",
      "     -                                        0.6/12.3 MB 7.1 MB/s eta 0:00:02\n",
      "     ---                                      0.9/12.3 MB 7.5 MB/s eta 0:00:02\n",
      "     ----                                     1.4/12.3 MB 8.0 MB/s eta 0:00:02\n",
      "     ------                                   1.9/12.3 MB 8.6 MB/s eta 0:00:02\n",
      "     ------                                   2.1/12.3 MB 7.8 MB/s eta 0:00:02\n",
      "     -------                                  2.4/12.3 MB 7.7 MB/s eta 0:00:02\n",
      "     ---------                                2.9/12.3 MB 8.1 MB/s eta 0:00:02\n",
      "     -----------                              3.4/12.3 MB 8.4 MB/s eta 0:00:02\n",
      "     ------------                             3.9/12.3 MB 8.5 MB/s eta 0:00:01\n",
      "     --------------                           4.4/12.3 MB 8.7 MB/s eta 0:00:01\n",
      "     ---------------                          4.9/12.3 MB 8.9 MB/s eta 0:00:01\n",
      "     ----------------                         5.0/12.3 MB 8.4 MB/s eta 0:00:01\n",
      "     -----------------                        5.3/12.3 MB 8.2 MB/s eta 0:00:01\n",
      "     ------------------                       5.6/12.3 MB 8.1 MB/s eta 0:00:01\n",
      "     ------------------                       5.8/12.3 MB 7.9 MB/s eta 0:00:01\n",
      "     -------------------                      6.1/12.3 MB 7.8 MB/s eta 0:00:01\n",
      "     --------------------                     6.4/12.3 MB 7.7 MB/s eta 0:00:01\n",
      "     ---------------------                    6.7/12.3 MB 7.7 MB/s eta 0:00:01\n",
      "     -----------------------                  7.1/12.3 MB 7.7 MB/s eta 0:00:01\n",
      "     -----------------------                  7.3/12.3 MB 7.5 MB/s eta 0:00:01\n",
      "     ------------------------                 7.6/12.3 MB 7.5 MB/s eta 0:00:01\n",
      "     -------------------------                8.0/12.3 MB 7.5 MB/s eta 0:00:01\n",
      "     --------------------------               8.2/12.3 MB 7.4 MB/s eta 0:00:01\n",
      "     ---------------------------              8.6/12.3 MB 7.4 MB/s eta 0:00:01\n",
      "     ----------------------------             8.9/12.3 MB 7.4 MB/s eta 0:00:01\n",
      "     -----------------------------            9.1/12.3 MB 7.3 MB/s eta 0:00:01\n",
      "     ------------------------------           9.5/12.3 MB 7.3 MB/s eta 0:00:01\n",
      "     --------------------------------         9.9/12.3 MB 7.4 MB/s eta 0:00:01\n",
      "     ---------------------------------        10.4/12.3 MB 7.5 MB/s eta 0:00:01\n",
      "     -----------------------------------      10.8/12.3 MB 7.7 MB/s eta 0:00:01\n",
      "     ------------------------------------     11.3/12.3 MB 7.7 MB/s eta 0:00:01\n",
      "     --------------------------------------   11.7/12.3 MB 7.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.3/12.3 MB 7.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.3/12.3 MB 7.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.3/12.3 MB 7.6 MB/s eta 0:00:00\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached murmurhash-1.0.9-cp39-cp39-win_amd64.whl (18 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached cymem-2.0.7-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.8-cp39-cp39-win_amd64.whl (96 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.8 (from spacy)\n",
      "  Downloading thinc-8.1.10-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "                                              0.0/1.5 MB ? eta -:--:--\n",
      "     -----------                              0.4/1.5 MB 8.5 MB/s eta 0:00:01\n",
      "     ---------------------                    0.8/1.5 MB 8.5 MB/s eta 0:00:01\n",
      "     -------------------------------          1.2/1.5 MB 8.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 7.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 7.2 MB/s eta 0:00:00\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Using cached srsly-2.4.6-cp39-cp39-win_amd64.whl (482 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Collecting pathy>=0.10.0 (from spacy)\n",
      "  Using cached pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy)\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda3\\envs\\nergpu\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\anaconda3\\envs\\nergpu\\lib\\site-packages (from spacy) (1.24.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda3\\envs\\nergpu\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-1.10.11-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "                                              0.0/2.2 MB ? eta -:--:--\n",
      "     ---------                                0.5/2.2 MB 10.7 MB/s eta 0:00:01\n",
      "     --------------                           0.8/2.2 MB 8.3 MB/s eta 0:00:01\n",
      "     ----------------------                   1.2/2.2 MB 9.6 MB/s eta 0:00:01\n",
      "     --------------------------------         1.8/2.2 MB 10.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.2/2.2 MB 9.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\envs\\nergpu\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\envs\\nergpu\\lib\\site-packages (from spacy) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda3\\envs\\nergpu\\lib\\site-packages (from spacy) (23.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in d:\\anaconda3\\envs\\nergpu\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\anaconda3\\envs\\nergpu\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\envs\\nergpu\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\envs\\nergpu\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\envs\\nergpu\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.2.0,>=8.1.8->spacy)\n",
      "  Using cached blis-0.7.9-cp39-cp39-win_amd64.whl (7.0 MB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.2.0,>=8.1.8->spacy)\n",
      "  Using cached confection-0.1.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\envs\\nergpu\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Collecting click<9.0.0,>=7.1.1 (from typer<0.10.0,>=0.3.0->spacy)\n",
      "  Downloading click-8.1.4-py3-none-any.whl (98 kB)\n",
      "                                              0.0/98.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 98.2/98.2 kB ? eta 0:00:00\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda3\\envs\\nergpu\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, pydantic, murmurhash, langcodes, click, catalogue, blis, typer, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 click-8.1.4 confection-0.1.0 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.2 preshed-3.0.8 pydantic-1.10.11 smart-open-6.3.0 spacy-3.6.0 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.10 typer-0.9.0 wasabi-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f9b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc21cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7d10cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68305580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install jsonl-to-conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4047a539",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcdbada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install doccano-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b774f10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install transformers datasets tokenizers segeval -q "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d5811",
   "metadata": {},
   "source": [
    "# Converting JSONL to ConnL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59c0f04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing to convert docanno dataset into ConLL\n",
    "from doccano_transformer.datasets import NERDataset\n",
    "from doccano_transformer.utils import read_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40765b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f92f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cb6828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "print(\"Current Directory:\", current_dir)\n",
    "data_file = os.path.join(current_dir,'Labelled Data','labelled_data.jsonl')\n",
    "print(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c93c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_jsonl(filepath=data_file, dataset=NERDataset, encoding='utf-8')\n",
    "conll_data = dataset.to_conll2003(tokenizer=str.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9128aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOI_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ce225",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for line in conll_data:\n",
    "    lines = line['data'].strip().split('\\n')\n",
    "    lines = [line for line in lines if line != '-DOCSTART- -X- -X- O' and line.strip() != '']\n",
    "    \n",
    "    result = []\n",
    "    for line in lines:\n",
    "        line = line.replace(\"_ _ \", \"\")\n",
    "        result.append(line)\n",
    "    cleaned_data = '\\n'.join(result)\n",
    "    \n",
    "    BOI_list.append(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b17e0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BOI_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d2c21b",
   "metadata": {},
   "source": [
    "# Saving data in 5 cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d197ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 5\n",
    "\n",
    "# kf = KFold(n_splits=k)\n",
    "\n",
    "# i = 0\n",
    "# for train_index, test_index in kf.split(BOI_list):\n",
    "#     train_data = [BOI_list[i] for i in train_index]\n",
    "#     test_data = [BOI_list[i] for i in test_index]\n",
    "#     data = {'train': train_data, 'test': test_data}\n",
    "    \n",
    "#     # Convert the dictionary to a JSON string\n",
    "#     json_data = json.dumps(data)\n",
    "    \n",
    "#     file_path = os.path.join(current_dir,'Labelled Data',f'dataset{i}.json')\n",
    "#     # Write the JSON string to a file\n",
    "#     with open(file_path, 'w') as file:\n",
    "#         file.write(json_data)\n",
    "    \n",
    "#     i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c2ce3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSON file\n",
    "\n",
    "i = 0\n",
    "current_dir = os.getcwd()\n",
    "file_path = os.path.join(current_dir,'Labelled Data',f'dataset{i}.json')\n",
    "with open(file_path, 'r') as file:\n",
    "    json_data = file.read()\n",
    "\n",
    "# Parse the JSON string into a dictionary\n",
    "data = json.loads(json_data)\n",
    "\n",
    "# Retrieve the lists from the dictionary\n",
    "train_data = data['train']\n",
    "test_data = data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0b59ddc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618b124a",
   "metadata": {},
   "source": [
    "# Preparing ConLL data for the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c23ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "642449a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_sentences_and_labels(BOI_list):\n",
    "    # Initialize empty lists for sentences and word labels\n",
    "    sentences = []\n",
    "    word_labels = []\n",
    "\n",
    "    # Process each input string\n",
    "    for input_string in BOI_list:\n",
    "        lines = input_string.split(\"\\n\")\n",
    "        sentence = []\n",
    "        labels = []\n",
    "        for line in lines:\n",
    "            if line.strip() != \"\":\n",
    "                word, label = line.split(\" \")\n",
    "                sentence.append(word)\n",
    "                labels.append(label)\n",
    "        sentences.append(\" \".join(sentence))\n",
    "        word_labels.append(\",\".join(labels))\n",
    "\n",
    "    # Create pandas DataFrame\n",
    "    data_style = {\"sentence\": sentences, \"word_labels\": word_labels}\n",
    "    data = pd.DataFrame(data_style)\n",
    "    \n",
    "    return sentences, word_labels, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f948ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, word_labels, data = get_sentences_and_labels(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91c46fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "146c72dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Practices like CI/CD and automation have becom...</td>\n",
       "      <td>O,O,B-Development_Scalability,O,B-Development_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Research, on behalf of Atlassian, conducted an...</td>\n",
       "      <td>O,O,O,O,B-Company_Name,O,O,O,O,O,B-Internal_Or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On call improves the work product of developer...</td>\n",
       "      <td>O,O,O,O,O,O,O,B-Internal_Organization,I-Intern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Traditionally, many organizations have dedicat...</td>\n",
       "      <td>O,O,O,O,O,B-Internal_Organization,I-Internal_O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A relatively new role popularized by Google, s...</td>\n",
       "      <td>O,O,O,O,O,O,B-Company_Name,B-Internal_Organiza...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0  Practices like CI/CD and automation have becom...   \n",
       "1  Research, on behalf of Atlassian, conducted an...   \n",
       "2  On call improves the work product of developer...   \n",
       "3  Traditionally, many organizations have dedicat...   \n",
       "4  A relatively new role popularized by Google, s...   \n",
       "\n",
       "                                         word_labels  \n",
       "0  O,O,B-Development_Scalability,O,B-Development_...  \n",
       "1  O,O,O,O,B-Company_Name,O,O,O,O,O,B-Internal_Or...  \n",
       "2  O,O,O,O,O,O,O,B-Internal_Organization,I-Intern...  \n",
       "3  O,O,O,O,O,B-Internal_Organization,I-Internal_O...  \n",
       "4  O,O,O,O,O,O,B-Company_Name,B-Internal_Organiza...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "338f5756",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = set()\n",
    "for input_string in train_data:\n",
    "    lines = input_string.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        if line.strip() != \"\":\n",
    "            word, label = line.split(\" \")\n",
    "            all_labels.add(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97e54044",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I-Data_Scalability': 0,\n",
       " 'I-Company_Name': 1,\n",
       " 'I-Internal_Organization': 2,\n",
       " 'I-Userbase_Information': 3,\n",
       " 'B-Transaction_Scalability': 4,\n",
       " 'I-Development_Scalability': 5,\n",
       " 'B-Userbase_Information': 6,\n",
       " 'I-Software_Purpose': 7,\n",
       " 'O': 8,\n",
       " 'B-Software_Name': 9,\n",
       " 'I-Transaction_Scalability': 10,\n",
       " 'B-Software_Purpose': 11,\n",
       " 'B-Development_Scalability': 12,\n",
       " 'B-Company_Name': 13,\n",
       " 'B-Data_Scalability': 14,\n",
       " 'I-Software_Name': 15,\n",
       " 'B-Internal_Organization': 16}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_ids = {k: v for v, k in enumerate(all_labels)}\n",
    "labels_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43b45ab9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'I-Data_Scalability',\n",
       " 1: 'I-Company_Name',\n",
       " 2: 'I-Internal_Organization',\n",
       " 3: 'I-Userbase_Information',\n",
       " 4: 'B-Transaction_Scalability',\n",
       " 5: 'I-Development_Scalability',\n",
       " 6: 'B-Userbase_Information',\n",
       " 7: 'I-Software_Purpose',\n",
       " 8: 'O',\n",
       " 9: 'B-Software_Name',\n",
       " 10: 'I-Transaction_Scalability',\n",
       " 11: 'B-Software_Purpose',\n",
       " 12: 'B-Development_Scalability',\n",
       " 13: 'B-Company_Name',\n",
       " 14: 'B-Data_Scalability',\n",
       " 15: 'I-Software_Name',\n",
       " 16: 'B-Internal_Organization'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_to_labels = {id: tag for tag, id in labels_to_ids.items()}\n",
    "ids_to_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c467fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_ids = {'B-Development_Scalability': 0,\n",
    " 'B-Company_Name': 1,\n",
    " 'I-Internal_Organization': 2,\n",
    " 'O': 3,\n",
    " 'I-Software_Name': 4,\n",
    " 'B-Transaction_Scalability': 5,\n",
    " 'I-Company_Name': 6,\n",
    " 'I-Data_Scalability': 7,\n",
    " 'I-Software_Purpose': 8,\n",
    " 'I-Transaction_Scalability': 9,\n",
    " 'I-Userbase_Information': 10,\n",
    " 'B-Software_Purpose': 11,\n",
    " 'I-Development_Scalability': 12,\n",
    " 'B-Userbase_Information': 13,\n",
    " 'B-Software_Name': 14,\n",
    " 'B-Data_Scalability': 15,\n",
    " 'B-Internal_Organization': 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35eae0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_labels = {0: 'B-Development_Scalability',\n",
    " 1: 'B-Company_Name',\n",
    " 2: 'I-Internal_Organization',\n",
    " 3: 'O',\n",
    " 4: 'I-Software_Name',\n",
    " 5: 'B-Transaction_Scalability',\n",
    " 6: 'I-Company_Name',\n",
    " 7: 'I-Data_Scalability',\n",
    " 8: 'I-Software_Purpose',\n",
    " 9: 'I-Transaction_Scalability',\n",
    " 10: 'I-Userbase_Information',\n",
    " 11: 'B-Software_Purpose',\n",
    " 12: 'I-Development_Scalability',\n",
    " 13: 'B-Userbase_Information',\n",
    " 14: 'B-Software_Name',\n",
    " 15: 'B-Data_Scalability',\n",
    " 16: 'B-Internal_Organization'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45715cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = ['B-Development_Scalability',\n",
    " 'B-Company_Name',\n",
    " 'I-Internal_Organization',\n",
    " 'O',\n",
    " 'I-Software_Name',\n",
    " 'B-Transaction_Scalability',\n",
    " 'I-Company_Name',\n",
    " 'I-Data_Scalability',\n",
    " 'I-Software_Purpose',\n",
    " 'I-Transaction_Scalability',\n",
    " 'I-Userbase_Information',\n",
    " 'B-Software_Purpose',\n",
    " 'I-Development_Scalability',\n",
    " 'B-Userbase_Information',\n",
    " 'B-Software_Name',\n",
    " 'B-Data_Scalability',\n",
    " 'B-Internal_Organization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2872ebe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_texts_and_tags(BOI_list):\n",
    "    # Initialize empty lists for sentences and word labels\n",
    "    texts = []\n",
    "    tags = []\n",
    "\n",
    "    # Process each input string\n",
    "    for input_string in BOI_list:\n",
    "        lines = input_string.split(\"\\n\")\n",
    "        sentence = []\n",
    "        labels = []\n",
    "        for line in lines:\n",
    "            if line.strip() != \"\":\n",
    "                word, label = line.split(\" \")\n",
    "                sentence.append(word)\n",
    "                labels.append(labels_to_ids[label])\n",
    "        texts.append(sentence)\n",
    "        tags.append(labels)\n",
    "    \n",
    "    return texts, tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20c8a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, tags = get_texts_and_tags(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fe779dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts, test_tags = get_texts_and_tags(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ffc2b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into training and validation data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d85a81fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccc6ca43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67c99b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset(texts, tags):\n",
    "    data = {}\n",
    "    data[\"id\"] = [x for x in range(0,len(texts))]\n",
    "    data[\"tokens\"] = texts\n",
    "    data[\"ner_tags\"] = tags\n",
    "    return Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b89499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = construct_dataset(train_texts, train_tags)\n",
    "val_dataset = construct_dataset(val_texts, val_tags)\n",
    "test_dataset = construct_dataset(test_texts, test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1f1aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DatasetDict({\"train\": train_dataset, \"validation\": val_dataset, \"test\": test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6865c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 536\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 60\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 149\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1e48d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rest of the tokenization is based on this source\n",
    "# https://huggingface.co/docs/transformers/tasks/token_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1376f0ac",
   "metadata": {},
   "source": [
    "# Preparing the dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad118d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5a4b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03f44ce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AWS', 'implements', 'formal,', 'documented', 'policies', 'and', 'procedures', 'that', 'provide', 'guidance', 'for', 'operations', 'and', 'information', 'security', 'within', 'the', 'organization', 'and', 'the', 'supporting', 'AWS', 'environments.', 'Policies', 'address', 'purpose,', 'scope,', 'roles,', 'responsibilities', 'and', 'management', 'commitment.', 'All', 'policies', 'are', 'maintained', 'in', 'a', 'centralized', 'location', 'that', 'is', 'accessible', 'by', 'employees.']\n"
     ]
    }
   ],
   "source": [
    "example = datasets[\"train\"][1]\n",
    "print(example[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5d71626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 4, 3, 0, 12, 3, 0, 12, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 12, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 16]\n"
     ]
    }
   ],
   "source": [
    "print(example[\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4ba3fd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '‚ñÅa', 'ws', '‚ñÅimplement', 's', '‚ñÅformal', ',', '‚ñÅdocumented', '‚ñÅpolicies', '‚ñÅand', '‚ñÅprocedures', '‚ñÅthat', '‚ñÅprovide', '‚ñÅguidance', '‚ñÅfor', '‚ñÅoperations', '‚ñÅand', '‚ñÅinformation', '‚ñÅsecurity', '‚ñÅwithin', '‚ñÅthe', '‚ñÅorganization', '‚ñÅand', '‚ñÅthe', '‚ñÅsupporting', '‚ñÅa', 'ws', '‚ñÅenvironments', '.', '‚ñÅpolicies', '‚ñÅaddress', '‚ñÅpurpose', ',', '‚ñÅscope', ',', '‚ñÅroles', ',', '‚ñÅ', 'responsibilities', '‚ñÅand', '‚ñÅmanagement', '‚ñÅcommitment', '.', '‚ñÅall', '‚ñÅpolicies', '‚ñÅare', '‚ñÅmaintained', '‚ñÅin', '‚ñÅa', '‚ñÅcentralized', '‚ñÅlocation', '‚ñÅthat', '‚ñÅis', '‚ñÅaccessible', '‚ñÅby', '‚ñÅemployees', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "541118e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 58\n"
     ]
    }
   ],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[\"ner_tags\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5230e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 1, 1, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 21, 22, 22, 23, 24, 25, 25, 26, 26, 27, 27, 28, 28, 29, 30, 31, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 44, None]\n"
     ]
    }
   ],
   "source": [
    "print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7eaabab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 14,\n",
       " 14,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 12,\n",
       " 3,\n",
       " 0,\n",
       " 12,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 12,\n",
       " 12,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 16,\n",
       " 16,\n",
       " -100]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c99d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3657dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1f1260b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 158, 4823, 3501, 21, 2592, 4024, 128, 14, 10058, 15, 1895, 173, 15, 5282, 118, 19174, 11747, 15, 17, 2455, 329, 11747, 9, 3], [2, 21, 10268, 8713, 18, 3337, 15, 8926, 4845, 17, 8876, 30, 1181, 8193, 26, 1311, 17, 676, 1221, 363, 14, 1165, 17, 14, 3134, 21, 10268, 11246, 9, 4845, 3218, 2131, 15, 9914, 15, 2954, 15, 13, 10525, 17, 1097, 6578, 9, 65, 4845, 50, 3926, 19, 21, 25176, 1474, 30, 25, 7342, 34, 3716, 9, 3], [2, 11817, 18, 6127, 50, 3638, 71, 20, 21, 4496, 16, 331, 13, 11872, 16, 600, 8582, 726, 416, 4851, 13, 5, 3970, 14, 4851, 1, 18, 375, 4326, 6, 9, 3], [2, 32, 1927, 1516, 8, 23702, 17, 10719, 3186, 579, 2301, 133, 3108, 79, 1880, 85, 8, 17601, 1603, 8674, 145, 28, 7610, 8738, 68, 15, 6018, 18161, 15, 7331, 68, 17, 10119, 18, 9, 3], [2, 7974, 414, 8330, 129, 4077, 4721, 5350, 7951, 20, 953, 17, 3343, 7951, 9, 1086, 15, 931, 2443, 1880, 414, 20, 21, 64, 8, 1898, 28804, 491, 44, 21, 9857, 274, 19, 14, 704, 876, 76, 5350, 1238, 13328, 50, 117, 28, 12760, 2985, 4150, 8, 1738, 4734, 1373, 10926, 9, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 3, 3, 3, 3, 3, 3, 3, 3, 16, 16, 16, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, -100], [-100, 14, 14, 4, 4, 3, 3, 0, 12, 3, 0, 12, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 12, 12, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 16, 16, -100], [-100, 15, 15, 7, 3, 3, 3, 3, 3, 15, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 15, 15, 15, 7, 7, 7, 7, -100], [-100, 3, 3, 3, 3, 3, 3, 5, 5, 5, 9, 3, 0, 0, 0, 12, 12, 12, 12, 12, 3, 3, 3, 3, 3, 3, 15, 7, 7, 3, 3, 3, 3, 3, 3, -100], [-100, 3, 3, 3, 3, 3, 3, 0, 12, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 12, 3, 3, 3, 3, 3, 3, 3, 0, 12, 3, 0, 12, 12, 3, 3, 3, 16, 2, 0, 0, 0, 0, 0, 12, 12, -100]]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_align_labels(datasets['train'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c07f74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/536 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/149 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bca6749",
   "metadata": {},
   "source": [
    "# Importing the Albert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d7e2c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AlbertForTokenClassification \n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27338c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "076dd430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available: True\n",
      "CUDA device: NVIDIA GeForce GTX 1650 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
    "# True\n",
    "print(f\"CUDA device: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "# Tesla T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "060ffe09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "37d908a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"albert-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "722b7a87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForTokenClassification: ['predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.bias', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Importing the model\n",
    "model = AlbertForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "497e4d27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertForTokenClassification(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38f5a5a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    "    logging_steps = 20,\n",
    "    logging_dir='./logs',  # Directory for storing training logs\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc37eaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a149dd1c",
   "metadata": {},
   "source": [
    "# Compute metrics and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "441e9a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import json\n",
    "from transformers import pipeline, AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea7f1844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Athul Raj Nambiar\\AppData\\Local\\Temp\\ipykernel_11640\\152412463.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed4e3272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = evaluate.load('exact_match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51041d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Development_Scalability': {'precision': 1.0,\n",
       "  'recall': 1.0,\n",
       "  'f1': 1.0,\n",
       "  'number': 3},\n",
       " 'Internal_Organization': {'precision': 1.0,\n",
       "  'recall': 1.0,\n",
       "  'f1': 1.0,\n",
       "  'number': 1},\n",
       " 'Software_Name': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label_list[i] for i in example[\"ner_tags\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2186775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f7c14c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/athuln/albert-base-v2-finetuned-ner into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9afc9ed2a7437cb7b539e8d13b6840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file pytorch_model.bin:   0%|          | 16.5k/42.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b9d842b2c3b4e8fb32c3cee0cb89ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file training_args.bin: 100%|##########| 3.81k/3.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3191a30948747a8a14915fd6ec2ed34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file training_args.bin:  26%|##6       | 1.00k/3.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87737fc1a81b4371b9eaa1ee3ff848c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file pytorch_model.bin:   0%|          | 1.00k/42.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d895d174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\nergpu\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='670' max='670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [670/670 03:45, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.175800</td>\n",
       "      <td>1.026851</td>\n",
       "      <td>0.196891</td>\n",
       "      <td>0.114114</td>\n",
       "      <td>0.144487</td>\n",
       "      <td>0.711850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.801500</td>\n",
       "      <td>0.764348</td>\n",
       "      <td>0.327189</td>\n",
       "      <td>0.213213</td>\n",
       "      <td>0.258182</td>\n",
       "      <td>0.753826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.553700</td>\n",
       "      <td>0.642170</td>\n",
       "      <td>0.393491</td>\n",
       "      <td>0.399399</td>\n",
       "      <td>0.396423</td>\n",
       "      <td>0.803673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.452300</td>\n",
       "      <td>0.593323</td>\n",
       "      <td>0.431085</td>\n",
       "      <td>0.441441</td>\n",
       "      <td>0.436202</td>\n",
       "      <td>0.816791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>0.625691</td>\n",
       "      <td>0.404580</td>\n",
       "      <td>0.477477</td>\n",
       "      <td>0.438017</td>\n",
       "      <td>0.805422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.284300</td>\n",
       "      <td>0.610857</td>\n",
       "      <td>0.468391</td>\n",
       "      <td>0.489489</td>\n",
       "      <td>0.478708</td>\n",
       "      <td>0.814167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.631398</td>\n",
       "      <td>0.494318</td>\n",
       "      <td>0.522523</td>\n",
       "      <td>0.508029</td>\n",
       "      <td>0.821163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.163500</td>\n",
       "      <td>0.661203</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.525526</td>\n",
       "      <td>0.507246</td>\n",
       "      <td>0.816353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.141200</td>\n",
       "      <td>0.682776</td>\n",
       "      <td>0.480620</td>\n",
       "      <td>0.558559</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.814604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.085600</td>\n",
       "      <td>0.694013</td>\n",
       "      <td>0.482667</td>\n",
       "      <td>0.543544</td>\n",
       "      <td>0.511299</td>\n",
       "      <td>0.814604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\nergpu\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\Anaconda3\\envs\\nergpu\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=670, training_loss=0.45554525371807725, metrics={'train_runtime': 227.9132, 'train_samples_per_second': 23.518, 'train_steps_per_second': 2.94, 'total_flos': 19548914001552.0, 'train_loss': 0.45554525371807725, 'epoch': 10.0})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e68d94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"all_entites_1\"\n",
    "save_name = \"ner_model_\"+ version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "da06af8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 1.6129,\n",
       "  'learning_rate': 1.9402985074626868e-05,\n",
       "  'epoch': 0.3,\n",
       "  'step': 20},\n",
       " {'loss': 1.2785,\n",
       "  'learning_rate': 1.8805970149253735e-05,\n",
       "  'epoch': 0.6,\n",
       "  'step': 40},\n",
       " {'loss': 1.1758,\n",
       "  'learning_rate': 1.8208955223880598e-05,\n",
       "  'epoch': 0.9,\n",
       "  'step': 60},\n",
       " {'eval_loss': 1.0268510580062866,\n",
       "  'eval_precision': 0.19689119170984457,\n",
       "  'eval_recall': 0.11411411411411411,\n",
       "  'eval_f1': 0.1444866920152091,\n",
       "  'eval_accuracy': 0.7118495846086577,\n",
       "  'eval_runtime': 0.7669,\n",
       "  'eval_samples_per_second': 78.237,\n",
       "  'eval_steps_per_second': 10.432,\n",
       "  'epoch': 1.0,\n",
       "  'step': 67},\n",
       " {'loss': 1.001,\n",
       "  'learning_rate': 1.7611940298507464e-05,\n",
       "  'epoch': 1.19,\n",
       "  'step': 80},\n",
       " {'loss': 0.8433,\n",
       "  'learning_rate': 1.701492537313433e-05,\n",
       "  'epoch': 1.49,\n",
       "  'step': 100},\n",
       " {'loss': 0.8015,\n",
       "  'learning_rate': 1.6417910447761197e-05,\n",
       "  'epoch': 1.79,\n",
       "  'step': 120},\n",
       " {'eval_loss': 0.7643482685089111,\n",
       "  'eval_precision': 0.3271889400921659,\n",
       "  'eval_recall': 0.2132132132132132,\n",
       "  'eval_f1': 0.2581818181818182,\n",
       "  'eval_accuracy': 0.7538259728902492,\n",
       "  'eval_runtime': 0.7776,\n",
       "  'eval_samples_per_second': 77.16,\n",
       "  'eval_steps_per_second': 10.288,\n",
       "  'epoch': 2.0,\n",
       "  'step': 134},\n",
       " {'loss': 0.7589,\n",
       "  'learning_rate': 1.582089552238806e-05,\n",
       "  'epoch': 2.09,\n",
       "  'step': 140},\n",
       " {'loss': 0.6672,\n",
       "  'learning_rate': 1.5223880597014925e-05,\n",
       "  'epoch': 2.39,\n",
       "  'step': 160},\n",
       " {'loss': 0.6375,\n",
       "  'learning_rate': 1.4626865671641792e-05,\n",
       "  'epoch': 2.69,\n",
       "  'step': 180},\n",
       " {'loss': 0.5537,\n",
       "  'learning_rate': 1.4029850746268658e-05,\n",
       "  'epoch': 2.99,\n",
       "  'step': 200},\n",
       " {'eval_loss': 0.6421704888343811,\n",
       "  'eval_precision': 0.39349112426035504,\n",
       "  'eval_recall': 0.3993993993993994,\n",
       "  'eval_f1': 0.39642324888226527,\n",
       "  'eval_accuracy': 0.8036729339746392,\n",
       "  'eval_runtime': 0.7807,\n",
       "  'eval_samples_per_second': 76.856,\n",
       "  'eval_steps_per_second': 10.247,\n",
       "  'epoch': 3.0,\n",
       "  'step': 201},\n",
       " {'loss': 0.4954,\n",
       "  'learning_rate': 1.3432835820895525e-05,\n",
       "  'epoch': 3.28,\n",
       "  'step': 220},\n",
       " {'loss': 0.4969,\n",
       "  'learning_rate': 1.2835820895522388e-05,\n",
       "  'epoch': 3.58,\n",
       "  'step': 240},\n",
       " {'loss': 0.4523,\n",
       "  'learning_rate': 1.2238805970149255e-05,\n",
       "  'epoch': 3.88,\n",
       "  'step': 260},\n",
       " {'eval_loss': 0.5933228731155396,\n",
       "  'eval_precision': 0.4310850439882698,\n",
       "  'eval_recall': 0.44144144144144143,\n",
       "  'eval_f1': 0.43620178041543023,\n",
       "  'eval_accuracy': 0.8167905553126367,\n",
       "  'eval_runtime': 0.8051,\n",
       "  'eval_samples_per_second': 74.522,\n",
       "  'eval_steps_per_second': 9.936,\n",
       "  'epoch': 4.0,\n",
       "  'step': 268},\n",
       " {'loss': 0.4264,\n",
       "  'learning_rate': 1.1641791044776121e-05,\n",
       "  'epoch': 4.18,\n",
       "  'step': 280},\n",
       " {'loss': 0.3379,\n",
       "  'learning_rate': 1.1044776119402986e-05,\n",
       "  'epoch': 4.48,\n",
       "  'step': 300},\n",
       " {'loss': 0.385,\n",
       "  'learning_rate': 1.0447761194029851e-05,\n",
       "  'epoch': 4.78,\n",
       "  'step': 320},\n",
       " {'eval_loss': 0.6256913542747498,\n",
       "  'eval_precision': 0.40458015267175573,\n",
       "  'eval_recall': 0.4774774774774775,\n",
       "  'eval_f1': 0.4380165289256199,\n",
       "  'eval_accuracy': 0.805421950153039,\n",
       "  'eval_runtime': 0.7856,\n",
       "  'eval_samples_per_second': 76.373,\n",
       "  'eval_steps_per_second': 10.183,\n",
       "  'epoch': 5.0,\n",
       "  'step': 335},\n",
       " {'loss': 0.3399,\n",
       "  'learning_rate': 9.850746268656717e-06,\n",
       "  'epoch': 5.07,\n",
       "  'step': 340},\n",
       " {'loss': 0.2875,\n",
       "  'learning_rate': 9.253731343283582e-06,\n",
       "  'epoch': 5.37,\n",
       "  'step': 360},\n",
       " {'loss': 0.2666,\n",
       "  'learning_rate': 8.656716417910447e-06,\n",
       "  'epoch': 5.67,\n",
       "  'step': 380},\n",
       " {'loss': 0.2843,\n",
       "  'learning_rate': 8.059701492537314e-06,\n",
       "  'epoch': 5.97,\n",
       "  'step': 400},\n",
       " {'eval_loss': 0.6108571290969849,\n",
       "  'eval_precision': 0.46839080459770116,\n",
       "  'eval_recall': 0.4894894894894895,\n",
       "  'eval_f1': 0.47870778267254044,\n",
       "  'eval_accuracy': 0.8141670310450372,\n",
       "  'eval_runtime': 0.7903,\n",
       "  'eval_samples_per_second': 75.92,\n",
       "  'eval_steps_per_second': 10.123,\n",
       "  'epoch': 6.0,\n",
       "  'step': 402},\n",
       " {'loss': 0.2223,\n",
       "  'learning_rate': 7.46268656716418e-06,\n",
       "  'epoch': 6.27,\n",
       "  'step': 420},\n",
       " {'loss': 0.2191,\n",
       "  'learning_rate': 6.865671641791045e-06,\n",
       "  'epoch': 6.57,\n",
       "  'step': 440},\n",
       " {'loss': 0.2254,\n",
       "  'learning_rate': 6.2686567164179116e-06,\n",
       "  'epoch': 6.87,\n",
       "  'step': 460},\n",
       " {'eval_loss': 0.6313979625701904,\n",
       "  'eval_precision': 0.4943181818181818,\n",
       "  'eval_recall': 0.5225225225225225,\n",
       "  'eval_f1': 0.5080291970802919,\n",
       "  'eval_accuracy': 0.8211630957586358,\n",
       "  'eval_runtime': 0.7903,\n",
       "  'eval_samples_per_second': 75.92,\n",
       "  'eval_steps_per_second': 10.123,\n",
       "  'epoch': 7.0,\n",
       "  'step': 469},\n",
       " {'loss': 0.2041,\n",
       "  'learning_rate': 5.671641791044776e-06,\n",
       "  'epoch': 7.16,\n",
       "  'step': 480},\n",
       " {'loss': 0.1448,\n",
       "  'learning_rate': 5.074626865671642e-06,\n",
       "  'epoch': 7.46,\n",
       "  'step': 500},\n",
       " {'loss': 0.1635,\n",
       "  'learning_rate': 4.477611940298508e-06,\n",
       "  'epoch': 7.76,\n",
       "  'step': 520},\n",
       " {'eval_loss': 0.6612030863761902,\n",
       "  'eval_precision': 0.49019607843137253,\n",
       "  'eval_recall': 0.5255255255255256,\n",
       "  'eval_f1': 0.5072463768115942,\n",
       "  'eval_accuracy': 0.8163533012680367,\n",
       "  'eval_runtime': 0.7963,\n",
       "  'eval_samples_per_second': 75.349,\n",
       "  'eval_steps_per_second': 10.047,\n",
       "  'epoch': 8.0,\n",
       "  'step': 536},\n",
       " {'loss': 0.1796,\n",
       "  'learning_rate': 3.8805970149253735e-06,\n",
       "  'epoch': 8.06,\n",
       "  'step': 540},\n",
       " {'loss': 0.1266,\n",
       "  'learning_rate': 3.283582089552239e-06,\n",
       "  'epoch': 8.36,\n",
       "  'step': 560},\n",
       " {'loss': 0.1354,\n",
       "  'learning_rate': 2.686567164179105e-06,\n",
       "  'epoch': 8.66,\n",
       "  'step': 580},\n",
       " {'loss': 0.1412,\n",
       "  'learning_rate': 2.08955223880597e-06,\n",
       "  'epoch': 8.96,\n",
       "  'step': 600},\n",
       " {'eval_loss': 0.6827758550643921,\n",
       "  'eval_precision': 0.4806201550387597,\n",
       "  'eval_recall': 0.5585585585585585,\n",
       "  'eval_f1': 0.5166666666666666,\n",
       "  'eval_accuracy': 0.8146042850896371,\n",
       "  'eval_runtime': 0.7942,\n",
       "  'eval_samples_per_second': 75.547,\n",
       "  'eval_steps_per_second': 10.073,\n",
       "  'epoch': 9.0,\n",
       "  'step': 603},\n",
       " {'loss': 0.1115,\n",
       "  'learning_rate': 1.4925373134328358e-06,\n",
       "  'epoch': 9.25,\n",
       "  'step': 620},\n",
       " {'loss': 0.123,\n",
       "  'learning_rate': 8.955223880597015e-07,\n",
       "  'epoch': 9.55,\n",
       "  'step': 640},\n",
       " {'loss': 0.0856,\n",
       "  'learning_rate': 2.9850746268656716e-07,\n",
       "  'epoch': 9.85,\n",
       "  'step': 660},\n",
       " {'eval_loss': 0.6940129995346069,\n",
       "  'eval_precision': 0.4826666666666667,\n",
       "  'eval_recall': 0.5435435435435435,\n",
       "  'eval_f1': 0.5112994350282486,\n",
       "  'eval_accuracy': 0.8146042850896371,\n",
       "  'eval_runtime': 0.8021,\n",
       "  'eval_samples_per_second': 74.8,\n",
       "  'eval_steps_per_second': 9.973,\n",
       "  'epoch': 10.0,\n",
       "  'step': 670},\n",
       " {'train_runtime': 227.9132,\n",
       "  'train_samples_per_second': 23.518,\n",
       "  'train_steps_per_second': 2.94,\n",
       "  'total_flos': 19548914001552.0,\n",
       "  'train_loss': 0.45554525371807725,\n",
       "  'epoch': 10.0,\n",
       "  'step': 670}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_data = trainer.state.log_history\n",
    "log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6c849e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'log/{save_name}.json', 'w') as f:\n",
    "    json.dump(log_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c127579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Model\n",
    "model.save_pretrained(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "87c909cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer\\\\tokenizer_config.json',\n",
       " 'tokenizer\\\\special_tokens_map.json',\n",
       " 'tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df78884",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    str(i): label for i,label in enumerate(label_list)\n",
    "}\n",
    "label2id = {\n",
    "    label: str(i) for i,label in enumerate(label_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b63699",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open(f\"{save_name}/config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ebad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"id2label\"] = id2label\n",
    "config[\"label2id\"] = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17faafb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(config, open(f\"{save_name}/config.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650dcb2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_fine_tuned = AutoModelForTokenClassification.from_pretrained(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbd425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"ner\", model=model_fine_tuned, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "example = \"Google has 100 users for their cloud platform.\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc2fec6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
